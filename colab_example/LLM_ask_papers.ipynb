{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WIhlY_tRqOcn",
        "Hle6_Mbmrnk-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Libraries"
      ],
      "metadata": {
        "id": "h7AMvdMkRUW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/michalkrawczyk/Arxiv_GPT_Summarizer.git\n",
        "%cd Arxiv_GPT_Summarizer\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqvql9x88dQn",
        "outputId": "4184dded-43d3-4481-a164-bf4cd76c7580"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Arxiv_GPT_Summarizer'...\n",
            "remote: Enumerating objects: 567, done.\u001b[K\n",
            "remote: Counting objects: 100% (170/170), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 567 (delta 89), reused 137 (delta 64), pack-reused 397\u001b[K\n",
            "Receiving objects: 100% (567/567), 1.89 MiB | 11.12 MiB/s, done.\n",
            "Resolving deltas: 100% (294/294), done.\n",
            "/content/Arxiv_GPT_Summarizer\n",
            "Branch 'langchain' set up to track remote branch 'langchain' from 'origin'.\n",
            "Switched to a new branch 'langchain'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install .\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Libraries Installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcQgbXfeSXoQ",
        "outputId": "da167f73-d4cb-4f6c-97c8-50cb4f0bce9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries Installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import json\n",
        "import os\n",
        "from shutil import move\n",
        "from time import sleep\n",
        "import yaml\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "UNaahvK7-o4I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LOAD you openai API key and run cell below"
      ],
      "metadata": {
        "id": "MC1FgN934BaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import openai\n",
        "\n",
        "    with open(\"/content/Arxiv_GPT_Summarizer/openai_key.yaml\", \"r\") as f:\n",
        "        API_KEY = yaml.safe_load(f)[\"openai_api_key\"]\n",
        "        openai.api_key = API_KEY if API_KEY != \"OPENAI_API_KEY\" else  userdata.get(\"openai_key\")\n",
        "\n",
        "        OPENAI_AVAILABLE = bool(openai.api_key) and openai.api_key != \"OPENAI_API_KEY\"\n",
        "\n",
        "except Exception as err:\n",
        "    OPENAI_AVAILABLE = False\n",
        "    print(err)\n",
        "\n",
        "print(\"OPENAI loaded:\", OPENAI_AVAILABLE)\n",
        "assert OPENAI_AVAILABLE, \"OpenAI not available - check the key\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRfYkynm5NTi",
        "outputId": "196e0feb-ae75-4d54-992f-1d425faca43e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPENAI loaded: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Prompts\n",
        "PromptHolder class is used to store or create predefined langchain prompts for later usage.\n",
        "\n",
        "It is created to provide invidual set of prompts for different PaperDatasetLC instances\n",
        "\n",
        "(among others for different models or purposes - e.g. for one dataset with medical topics and one for financial).\n",
        "<br><br>\n",
        "If user don't want to use multiple instances,\n",
        "he can use default prompt holder (DEFAULT_PROMPT_REGISTER) from 'templates' module,\n",
        "which is used by default when no PromptHolder is provided."
      ],
      "metadata": {
        "id": "JId5izP36Xtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from templates import create_and_register_prompt, register_prompt, PromptHolder, DEFAULT_PROMPT_REGISTER"
      ],
      "metadata": {
        "id": "zFigupg9ikD8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Register Prompts\n",
        "Those register functions are provided to quickly register prompts to given prompt holder.\n",
        "\n",
        "They're outside PromptHolder class, as they are also made as convenient shortcuts to add prompts to default prompt holder."
      ],
      "metadata": {
        "id": "VZzWXMSzaIDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_prompt = \"\"\"Create short, specific summary for research paper. Identify the following items for given text:\n",
        "  - Model Name\n",
        "  - Model category(e.g Object Detection, NLP or image generation)\n",
        "  - SOTA: if Model is State-of-the-Art\n",
        "  - New Features: Introduced new model components, layers or other features, as keywords\n",
        "  - New Strategies: New introduced learning strategies\n",
        "  - Year: Year of publishing\n",
        "\n",
        "  text: {text}\n",
        "\n",
        "  {format_instructions}\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "5fOZ97OWpQjV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_and_register_prompt(name=\"identify_features\", template=features_prompt, input_variables=[\"text\", \"format_instructions\"])"
      ],
      "metadata": {
        "id": "saBnfrI3psqP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "is equivalent to:"
      ],
      "metadata": {
        "id": "FwcYodqEp5zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(template=features_prompt, input_variables=[\"text\", \"format_instructions\"])\n",
        "DEFAULT_PROMPT_REGISTER.load_defined_prompt(name=\"identify_features\", prompt=prompt)"
      ],
      "metadata": {
        "id": "VDSMMwXvptbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1229ae3-9128-4040-e3b0-21a00c998644"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:templates.prompt_holders:Prompt identify_features already defined, skipping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "or"
      ],
      "metadata": {
        "id": "tXN24HMCrTjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(template=features_prompt, input_variables=[\"text\", \"format_instructions\"])\n",
        "register_prompt(name=\"identify_features\", prompt=prompt)"
      ],
      "metadata": {
        "id": "ZgMAF7yJrSFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fd3220-0261-44a8-d41a-4af231e1ad35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:templates.prompt_holders:Prompt identify_features already defined, skipping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO:PromptHolder other than default\n",
        "#TODO: Add in project option to load prompt from yaml"
      ],
      "metadata": {
        "id": "jZ0695s86XH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Arxiv Utils"
      ],
      "metadata": {
        "id": "h-w7NfPMtOax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from arxiv_utils import download_paper_from_arxiv, download_recent_papers_by_querry\n",
        "#TODO: Examples"
      ],
      "metadata": {
        "id": "UyLOaq7ItSCN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper Dataset\n",
        "By default PaperDatasetLC is initialized with ChromaDB with OpenAI embeddings(\"text-embedding-ada-002\") and text-davinci-003 model but can be initialized with any model and embedding loaded via langchain"
      ],
      "metadata": {
        "id": "gBPjUMrTtxIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import PaperDatasetLC\n",
        "\n",
        "dataset = PaperDatasetLC()\n",
        "# dataset = PaperDatasetLC(db=Chroma(embedding_function=embeddings), llm=model)"
      ],
      "metadata": {
        "id": "0WqwsZ2yt5yM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c354067-2cf6-47d5-b7dd-f036ccd62ad0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.paper_dataset:Dataset with not specified or invalid database - using Chroma with OpenAI embeddings and LLM\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py:268: UserWarning: WARNING! collection_metadata is not default parameter.\n",
            "                    collection_metadata was transferred to model_kwargs.\n",
            "                    Please confirm that collection_metadata is what you intended.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "WARNING:datasets.paper_dataset:Prompt summarize_doc not defined - Some functions using it, may not work or require changing prompt in parameters \n",
            "WARNING:datasets.paper_dataset:Prompt summarize_doc_refine not defined - Some functions using it, may not work or require changing prompt in parameters \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding documents\n",
        "Notes:\n",
        "-  Every added document return id in database on creation, for easier later search\n",
        "- Optional metadata, by now, only fills missing document metadata items"
      ],
      "metadata": {
        "id": "uK5Jl7gJwGiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langchain documents\n",
        "Adding Document class from langchain"
      ],
      "metadata": {
        "id": "iDit5Vm2zrOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.add_document\n",
        "# dataset.add_documents"
      ],
      "metadata": {
        "id": "A7b9l7tIq-fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF files"
      ],
      "metadata": {
        "id": "SDAW9mEO0NMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_ids = dataset.add_pdf_file(\"/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf\", metadata=None)\n",
        "doc_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4caPUQXlyHqD",
        "outputId": "3487e1c1-09a2-4d3b-ffaf-60d31d294126"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5c6982b0-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698472-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698508-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698580-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c6985f8-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698670-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c6986e8-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698756-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c6987c4-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698832-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c6988aa-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698922-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698990-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c6989fe-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698ab2-dbf9-11ee-98e5-0242ac1c000c',\n",
              " '5c698b20-dbf9-11ee-98e5-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = [\"Lorem impsum something something\", \"Some Other Text\"]\n",
        "sample_metas = [{\"source\": \"sth\", \"v\": True}, {\"other\": \"sth\"}]\n",
        "\n",
        "\n",
        "dataset.add_texts(sample_text, sample_metas, skip_invalid=True) # only first text will be added due to missing 'source' value in metadata"
      ],
      "metadata": {
        "id": "Vtio55NdyLNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79aae5df-f854-4e80-e231-7f25c58eb67f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.paper_dataset:Paper Dataset not accept texts without specified source in metadata \n",
            "Index of problematic record: '1'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['605f4620-dbf9-11ee-98e5-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arxiv Papers"
      ],
      "metadata": {
        "id": "Prs5p-bJ0iRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Added document splits:\",\n",
        "      len(dataset.add_arxiv_by_id([\"1812.01187\", \"2207.02696\"])))\n",
        "#TODO: info about ExtendedArxivRetriever used id paper add"
      ],
      "metadata": {
        "id": "8Lj_KLDuyPFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93447e3-9b7f-4a97-914b-f0e17d7b620e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files...: 2it [00:01,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added document splits: 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # max_docs: int = 10\n",
        "    # top_k_results: int = 3\n",
        "    # sort_docs_by: SortCriterion = SortCriterion.Relevance\n",
        "    # sort_order: SortOrder = SortOrder.Descending\n",
        "    # load_all_available_meta: bool = False\n",
        "\n",
        "    # doc_content_chars_max: Union[int, None] = None\n",
        "    # ARXIV_MAX_QUERY_LENGTH: Union[int, None] = 300\n",
        "\n",
        "    # save_pdf: bool = True\n",
        "    # file_save_dir: str = \".\"\n",
        "    # overwrite_existing: bool = False"
      ],
      "metadata": {
        "id": "dnV0W40Ysx0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Added document splits:\",\n",
        "      len(dataset.add_arxiv_by_query(query=\"Yolov7\", max_docs=2)))"
      ],
      "metadata": {
        "id": "WpdEiVtfyW7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6856b6-ac51-4999-ccd8-0a0492937976"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files...: 2it [00:01,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added document splits: 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing functions"
      ],
      "metadata": {
        "id": "ocC1bkEIyyq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.unique_list_of_documents()"
      ],
      "metadata": {
        "id": "j7thJKdUQn-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68722f23-7f16-4c36-a3ea-0215a8a8785d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Listing documents: 100%|██████████| 134/134 [00:00<00:00, 126017.21it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('',\n",
              "  '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf',\n",
              "  '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf'),\n",
              " ('Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer',\n",
              "  '[http://arxiv.org/abs/2403.01736v1] Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer',\n",
              "  './2403.01736v1.Lightweight_Object_Detection__A_Study_Based_on_YOLOv7_Integrated_with_ShuffleNetv2_and_Vision_Transformer.pdf'),\n",
              " ('Bag of Tricks for Image Classification with Convolutional Neural Networks',\n",
              "  '[http://arxiv.org/abs/1812.01187v2] Bag of Tricks for Image Classification with Convolutional Neural Networks',\n",
              "  './1812.01187v2.Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks.pdf'),\n",
              " ('YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors',\n",
              "  '[http://arxiv.org/abs/2207.02696v1] YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors',\n",
              "  './2207.02696v1.YOLOv7__Trainable_bag_of_freebies_sets_new_state_of_the_art_for_real_time_object_detectors.pdf'),\n",
              " ('mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection',\n",
              "  '[http://arxiv.org/abs/2402.16291v1] mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection',\n",
              "  './2402.16291v1.mAPm__multi_scale_Attention_Pyramid_module_for_Enhanced_scale_variation_in_RLD_detection.pdf'),\n",
              " ('Unknown Text', 'sth', '')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.list_documents_by_id()"
      ],
      "metadata": {
        "id": "ezrDvmKwQr8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f6cfd7-495e-4906-d854-44f85611209b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('7209a640-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 1, part: 4'),\n",
              " ('69f36176-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 13, part: 34'),\n",
              " ('69f361e4-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 14, part: 35'),\n",
              " ('69f3632e-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 14, part: 36'),\n",
              " ('69f363a6-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 14, part: 37'),\n",
              " ('7209a190-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 0, part: 0'),\n",
              " ('7209a3e8-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 0, part: 1'),\n",
              " ('7209a4c4-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 0, part: 2'),\n",
              " ('7209a58c-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 1, part: 3'),\n",
              " ('69f36108-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 13, part: 33'),\n",
              " ('7209a6f4-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 1, part: 5'),\n",
              " ('7209a79e-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 1, part: 6'),\n",
              " ('7209a852-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 2, part: 7'),\n",
              " ('7209a8f2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 3, part: 8'),\n",
              " ('7209a99c-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 3, part: 9'),\n",
              " ('7209bde2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 16, part: 29'),\n",
              " ('7209be50-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 17, part: 30'),\n",
              " ('69f35d3e-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 8, part: 25'),\n",
              " ('69f359e2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 5, part: 17'),\n",
              " ('69f35a50-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 5, part: 18'),\n",
              " ('69f35abe-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 5, part: 19'),\n",
              " ('69f35b2c-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 6, part: 20'),\n",
              " ('69f35b90-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 6, part: 21'),\n",
              " ('69f35bfe-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 7, part: 22'),\n",
              " ('69f35c6c-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 7, part: 23'),\n",
              " ('69f35cd0-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 8, part: 24'),\n",
              " ('7209bebe-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 18, part: 31'),\n",
              " ('69f35dac-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 8, part: 26'),\n",
              " ('69f35e1a-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 9, part: 27'),\n",
              " ('69f35e88-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 11, part: 28'),\n",
              " ('69f35ef6-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 11, part: 29'),\n",
              " ('69f35fa0-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 11, part: 30'),\n",
              " ('69f36018-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 12, part: 31'),\n",
              " ('69f3607c-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 12, part: 32'),\n",
              " ('7209bbb2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 14, part: 24'),\n",
              " ('7209b838-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 13, part: 16'),\n",
              " ('7209b8a6-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 13, part: 17'),\n",
              " ('7209b914-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 13, part: 18'),\n",
              " ('7209b978-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 13, part: 19'),\n",
              " ('7209b9e6-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 13, part: 20'),\n",
              " ('7209ba54-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 14, part: 21'),\n",
              " ('7209bac2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 14, part: 22'),\n",
              " ('7209bb44-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 14, part: 23'),\n",
              " ('7209b7c0-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 13, part: 15'),\n",
              " ('7209bc20-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 14, part: 25'),\n",
              " ('7209bc8e-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 14, part: 26'),\n",
              " ('7209bd06-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 15, part: 27'),\n",
              " ('7209bd74-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 16, part: 28'),\n",
              " ('7209aa46-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 3, part: 10'),\n",
              " ('7209aae6-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 3, part: 11'),\n",
              " ('5c6982b0-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 0, part: 0'),\n",
              " ('69f348bc-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 4, part: 17'),\n",
              " ('7209b70c-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 12, part: 14'),\n",
              " ('7209b694-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 10, part: 13'),\n",
              " ('7209b626-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 8, part: 12'),\n",
              " ('7209b59a-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 8, part: 11'),\n",
              " ('7209b4dc-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 7, part: 10'),\n",
              " ('7209b432-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 6, part: 9'),\n",
              " ('7209b37e-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 5, part: 8'),\n",
              " ('7209b2ac-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 4, part: 7'),\n",
              " ('7209b180-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 4, part: 6'),\n",
              " ('7209b0cc-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 3, part: 5'),\n",
              " ('7209b018-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 2, part: 4'),\n",
              " ('7209af6e-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 2, part: 3'),\n",
              " ('7209aeb0-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 1, part: 2'),\n",
              " ('7209acda-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 0, part: 1'),\n",
              " ('7209ac3a-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection - page: 0, part: 0'),\n",
              " ('69f3454c-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 1, part: 9'),\n",
              " ('69f3410a-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 0, part: 1'),\n",
              " ('69f341be-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 0, part: 2'),\n",
              " ('69f34236-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 1, part: 3'),\n",
              " ('69f342ae-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 1, part: 4'),\n",
              " ('69f34326-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 1, part: 5'),\n",
              " ('69f3439e-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 1, part: 6'),\n",
              " ('69f34466-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 1, part: 7'),\n",
              " ('69f344de-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 1, part: 8'),\n",
              " ('69f33f16-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 0, part: 0'),\n",
              " ('69f345ba-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 2, part: 10'),\n",
              " ('69f34628-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 2, part: 11'),\n",
              " ('69f34696-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 2, part: 12'),\n",
              " ('69f346fa-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 2, part: 13'),\n",
              " ('69f34768-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 3, part: 14'),\n",
              " ('69f347d6-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 3, part: 15'),\n",
              " ('69f34844-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 3, part: 16'),\n",
              " ('5c698832-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 2, part: 9'),\n",
              " ('7209ab90-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Lightweight Object Detection: A Study Based on YOLOv7 Integrated with ShuffleNetv2 and Vision Transformer - page: 4, part: 12'),\n",
              " ('69f3492a-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 4, part: 18'),\n",
              " ('69f34998-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 4, part: 19'),\n",
              " ('5c6985f8-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 1, part: 4'),\n",
              " ('5c698670-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 1, part: 5'),\n",
              " ('5c6986e8-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 1, part: 6'),\n",
              " ('5c698756-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 1, part: 7'),\n",
              " ('5c6987c4-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 2, part: 8'),\n",
              " ('5c698472-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 0, part: 1'),\n",
              " ('5c6988aa-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  '2302.00386.pdf - page: 2, part: 10'),\n",
              " ('5c698922-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  '2302.00386.pdf - page: 2, part: 11'),\n",
              " ('5c698990-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  '2302.00386.pdf - page: 3, part: 12'),\n",
              " ('5c6989fe-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  '2302.00386.pdf - page: 3, part: 13'),\n",
              " ('5c698ab2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  '2302.00386.pdf - page: 3, part: 14'),\n",
              " ('5c698b20-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  '2302.00386.pdf - page: 4, part: 15'),\n",
              " ('605f4620-dbf9-11ee-98e5-0242ac1c000c', 'sth - page: 0, part: 0'),\n",
              " ('69f353a2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 2, part: 8'),\n",
              " ('69f35014-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 0'),\n",
              " ('69f3508c-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 1'),\n",
              " ('69f350fa-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 2'),\n",
              " ('69f35172-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 1, part: 3'),\n",
              " ('69f351ea-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 1, part: 4'),\n",
              " ('69f35262-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 1, part: 5'),\n",
              " ('69f352c6-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 1, part: 6'),\n",
              " ('69f3533e-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 2, part: 7'),\n",
              " ('69f34fa6-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 9, part: 33'),\n",
              " ('69f35410-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 2, part: 9'),\n",
              " ('69f354e2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 2, part: 10'),\n",
              " ('69f355dc-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 3, part: 11'),\n",
              " ('69f3564a-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 3, part: 12'),\n",
              " ('69f356e0-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 4, part: 13'),\n",
              " ('69f357d0-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 4, part: 14'),\n",
              " ('69f358a2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 4, part: 15'),\n",
              " ('69f35960-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 4, part: 16'),\n",
              " ('69f34f42-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 8, part: 32'),\n",
              " ('69f34ed4-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 8, part: 31'),\n",
              " ('69f34e66-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 7, part: 30'),\n",
              " ('69f34dee-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 7, part: 29'),\n",
              " ('69f34d80-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 6, part: 28'),\n",
              " ('69f34d1c-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 6, part: 27'),\n",
              " ('69f34cae-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 6, part: 26'),\n",
              " ('69f34c40-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 6, part: 25'),\n",
              " ('69f34bd2-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 5, part: 24'),\n",
              " ('69f34b5a-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 5, part: 23'),\n",
              " ('69f34aec-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 5, part: 22'),\n",
              " ('69f34a6a-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 5, part: 21'),\n",
              " ('69f34a06-dbf9-11ee-98e5-0242ac1c000c',\n",
              "  'Bag of Tricks for Image Classification with Convolutional Neural Networks - page: 4, part: 20'),\n",
              " ('5c698580-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 1, part: 3'),\n",
              " ('5c698508-dbf9-11ee-98e5-0242ac1c000c', '2302.00386.pdf - page: 0, part: 2')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.list_available_fields()"
      ],
      "metadata": {
        "id": "TtMX61ASQ07O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a941b74d-a855-4c3c-b909-04f87382cc4b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Listing available fields: 100%|██████████| 134/134 [00:00<00:00, 161597.68it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['authors',\n",
              " 'total_pages',\n",
              " 'modDate',\n",
              " 'producer',\n",
              " 'split_part',\n",
              " 'source',\n",
              " 'trapped',\n",
              " 'date',\n",
              " 'v',\n",
              " 'file_path',\n",
              " 'page',\n",
              " 'title',\n",
              " 'subject',\n",
              " 'summary',\n",
              " 'published',\n",
              " 'author',\n",
              " 'keywords',\n",
              " 'creationDate',\n",
              " 'format',\n",
              " 'creator']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.get_by_id(doc_ids[-1], include=[\"metadatas\", \"documents\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Loul8GdE0pjl",
        "outputId": "3a9ea82d-3ed2-4e7d-8e55-804c952c803c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['5c698b20-dbf9-11ee-98e5-0242ac1c000c'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['Z. Li, and J. Sun, “YOLOX: exceeding YOLO\\nseries in 2021,” CoRR, vol. abs/2107.08430, 2021. III-A, III\\n[19] C. Wang, A. Bochkovskiy, and H. M. Liao, “Yolov7: Trainable bag-of-\\nfreebies sets new state-of-the-art for real-time object detectors,” CoRR,\\nvol. abs/2207.02696, 2022. III\\n[20] C. Wang, H. M. Liao, Y. Wu, P. Chen, J. Hsieh, and I. Yeh, “Cspnet: A\\nnew backbone that can enhance learning capability of CNN,” in 2020\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\nCVPR Workshops 2020, Seattle, WA, USA, June 14-19, 2020, pp. 1571–\\n1580, Computer Vision Foundation / IEEE, 2020. III-B\\n'],\n",
              " 'metadatas': [{'source': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf',\n",
              "   'file_path': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf',\n",
              "   'page': 4,\n",
              "   'total_pages': 5,\n",
              "   'format': 'PDF 1.5',\n",
              "   'title': '',\n",
              "   'author': '',\n",
              "   'subject': '',\n",
              "   'keywords': '',\n",
              "   'creator': 'LaTeX with hyperref',\n",
              "   'producer': 'pdfTeX-1.40.21',\n",
              "   'creationDate': 'D:20230202013154Z',\n",
              "   'modDate': 'D:20230202013154Z',\n",
              "   'trapped': '',\n",
              "   'split_part': 15}]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.get_containing_field(\"v\",  include=[\"metadatas\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXqCzOre1NsJ",
        "outputId": "db7e9185-d788-447c-bd8e-236cf30b3a97"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['605f4620-dbf9-11ee-98e5-0242ac1c000c'],\n",
              " 'embeddings': None,\n",
              " 'documents': None,\n",
              " 'metadatas': [{'source': 'sth',\n",
              "   'v': True,\n",
              "   'title': 'Unknown Text',\n",
              "   'split_part': 0}]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search Functions"
      ],
      "metadata": {
        "id": "NBEPa6fgzNEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.similarity_search(\"yolov7\", n_results=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-uX8UrJvE-q",
        "outputId": "e1d824db-f183-4f42-f4a7-daff88db2365"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='2015, San Diego, CA, USA, May 7-9,\\n2015, Conference Track Proceedings (Y. Bengio and Y. LeCun, eds.),\\n2015. II-A\\n[9] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov,\\nD. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with\\nconvolutions,” in IEEE Conference on Computer Vision and Pattern\\nRecognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pp. 1–9,\\nIEEE Computer Society, 2015. II-A\\n[10] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train one\\nnetwork and specialize it for efﬁcient deployment,” in 8th International\\nConference on Learning Representations, ICLR 2020, Addis Ababa,\\nEthiopia, April 26-30, 2020, OpenReview.net, 2020. II-B\\n[11] C. Li, L. Li, H. Jiang, K. Weng, Y. Geng, L. Li, Z. Ke, Q. Li, M. Cheng,\\nW. Nie, Y. Li, B. Zhang, Y. Liang, L. Zhou, X. Xu, X. Chu, X. Wei,\\nand X. Wei, “Yolov6: A single-stage object detection framework for\\nindustrial applications,” CoRR, vol. abs/2209.02976, 2022. III-A\\n[12] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, “You only\\nlook once: Uniﬁed, real-time object detection,” in 2016 IEEE Conference\\non Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas,\\n NV, USA, June 27-30, 2016, pp. 779–788, IEEE Computer Society, 2016.\\nIII-A\\n[13] J. Redmon and A. Farhadi, “YOLO9000: better, faster, stronger,” in\\n2017 IEEE Conference on Computer Vision and Pattern Recognition,\\nCVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 6517–6525, IEEE\\nComputer Society, 2017. III-A\\n[14] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”\\nCoRR, vol. abs/1804.02767, 2018. III-A\\n[15] A. Bochkovskiy, C. Wang, and H. M. Liao, “Yolov4: Optimal speed and\\naccuracy of object detection,” CoRR, vol. abs/2004.10934, 2020. III-A\\n[16] glenn jocher et al, “yolov5,” 2021. III-A, III-B, III\\n[17] S. Xu, X. Wang, W. Lv, Q. Chang, C. Cui, K. Deng, G. Wang, Q. Dang,\\nS. Wei, Y. Du, and B. Lai, “PP-YOLOE: an evolved version of YOLO,”\\nCoRR, vol. abs/2203.16250, 2022. III-A, III-B, III\\n[18] Z. Ge, S. Liu, F. Wang,', metadata={'source': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf', 'file_path': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf', 'page': 3, 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230202013154Z', 'modDate': 'D:20230202013154Z', 'trapped': '', 'split_part': 14})]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.similarity_search_with_scores(\"yolov7\", n_results=2, score_threshold=0.45)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBGoJ2D40T_t",
        "outputId": "6e07a278-104d-4ff9-e05f-6676c0c8e883"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(page_content='2015, San Diego, CA, USA, May 7-9,\\n2015, Conference Track Proceedings (Y. Bengio and Y. LeCun, eds.),\\n2015. II-A\\n[9] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov,\\nD. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with\\nconvolutions,” in IEEE Conference on Computer Vision and Pattern\\nRecognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pp. 1–9,\\nIEEE Computer Society, 2015. II-A\\n[10] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train one\\nnetwork and specialize it for efﬁcient deployment,” in 8th International\\nConference on Learning Representations, ICLR 2020, Addis Ababa,\\nEthiopia, April 26-30, 2020, OpenReview.net, 2020. II-B\\n[11] C. Li, L. Li, H. Jiang, K. Weng, Y. Geng, L. Li, Z. Ke, Q. Li, M. Cheng,\\nW. Nie, Y. Li, B. Zhang, Y. Liang, L. Zhou, X. Xu, X. Chu, X. Wei,\\nand X. Wei, “Yolov6: A single-stage object detection framework for\\nindustrial applications,” CoRR, vol. abs/2209.02976, 2022. III-A\\n[12] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, “You only\\nlook once: Uniﬁed, real-time object detection,” in 2016 IEEE Conference\\non Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas,\\n NV, USA, June 27-30, 2016, pp. 779–788, IEEE Computer Society, 2016.\\nIII-A\\n[13] J. Redmon and A. Farhadi, “YOLO9000: better, faster, stronger,” in\\n2017 IEEE Conference on Computer Vision and Pattern Recognition,\\nCVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 6517–6525, IEEE\\nComputer Society, 2017. III-A\\n[14] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”\\nCoRR, vol. abs/1804.02767, 2018. III-A\\n[15] A. Bochkovskiy, C. Wang, and H. M. Liao, “Yolov4: Optimal speed and\\naccuracy of object detection,” CoRR, vol. abs/2004.10934, 2020. III-A\\n[16] glenn jocher et al, “yolov5,” 2021. III-A, III-B, III\\n[17] S. Xu, X. Wang, W. Lv, Q. Chang, C. Cui, K. Deng, G. Wang, Q. Dang,\\nS. Wei, Y. Du, and B. Lai, “PP-YOLOE: an evolved version of YOLO,”\\nCoRR, vol. abs/2203.16250, 2022. III-A, III-B, III\\n[18] Z. Ge, S. Liu, F. Wang,', metadata={'source': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf', 'file_path': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf', 'page': 3, 'total_pages': 5, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230202013154Z', 'modDate': 'D:20230202013154Z', 'trapped': '', 'split_part': 14}),\n",
              "  1.2561867237091064),\n",
              " (Document(page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known\\nreal-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\\nDETR, DINO-5scale-R50, ViT-Adapter-B and many other\\nobject detectors in speed and accuracy. Moreover, we train\\nYOLOv7 only on MS COCO dataset from scratch without\\nusing any other datasets or pre-trained weights. Source\\ncode is released in https://github.com/WongKinYiu/yolov7.\\n1. Introduction\\n', metadata={'title': 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors', 'source': '[http://arxiv.org/abs/2207.02696v1] YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors', 'authors': 'Chien-Yao Wang, Alexey Bochkovskiy, Hong-Yuan Mark Liao', 'summary': 'YOLOv7 surpasses all known object detectors in both speed and accuracy in the\\nrange from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all\\nknown real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6\\nobject detector (56 FPS V100, 55.9% AP) outperforms both transformer-based\\ndetector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed\\nand 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask\\nR-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as\\nwell as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR,\\nDeformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors\\nin speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from\\nscratch without using any other datasets or pre-trained weights. Source code is\\nreleased in https://github.com/WongKinYiu/yolov7.', 'published': '2022-07-06', 'date': '2022-07-06', 'file_path': './2207.02696v1.YOLOv7__Trainable_bag_of_freebies_sets_new_state_of_the_art_for_real_time_object_detectors.pdf', 'page': 0, 'split_part': 0}),\n",
              "  1.331041932106018)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.update_document_features(doc_ids[1])\n",
        "dataset.get_containing_field(\"new_features\", include=[\"metadatas\"])[\"metadatas\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zanK6quH00S4",
        "outputId": "4cf83354-5eb2-4a11-995f-b331d82e9ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Updating metadata: 1it [00:01,  1.96s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'source': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf',\n",
              "  'file_path': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf',\n",
              "  'page': 0,\n",
              "  'total_pages': 5,\n",
              "  'format': 'PDF 1.5',\n",
              "  'title': '',\n",
              "  'author': '',\n",
              "  'subject': '',\n",
              "  'keywords': '',\n",
              "  'creator': 'LaTeX with hyperref',\n",
              "  'producer': 'pdfTeX-1.40.21',\n",
              "  'creationDate': 'D:20230202013154Z',\n",
              "  'modDate': 'D:20230202013154Z',\n",
              "  'trapped': '',\n",
              "  'split_part': 1,\n",
              "  'new_features.model_name': 'RepVGG',\n",
              "  'new_features.model_category': 'Object Detection',\n",
              "  'new_features.sota': 0,\n",
              "  'new_features.new_features': \"['3x3 convolutional kernel', 'winograd algorithm']\",\n",
              "  'new_features.new_strategies': \"['EfﬁcientRep backbone', 'Rep-PAN neck', 'Bep(Beer-mug) unit', 'BepC3(CSPStack-Rep) block']\",\n",
              "  'new_features.date': '2021',\n",
              "  'new_features.cls._type': 'ShortInfoSummary',\n",
              "  'new_features': 'metadata keys [new_features.cls._type, new_features.model_name, new_features.model_category, new_features.sota, new_features.new_features, new_features.new_strategies, new_features.date]'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "# from typing import List, Union\n",
        "# from pydantic import BaseModel, Field\n",
        "# class ShortInfoSummary(BaseModel):\n",
        "#     model_name: str = Field(\n",
        "#         description=\"Name of the model if provided\"\n",
        "#     )\n",
        "#     model_category: str = Field(\n",
        "#         description=\"Model Category (e.g Object Detection, NLP or image generation)\",\n",
        "\n",
        "#     )\n",
        "#     sota: int = Field(description=\"Boolean - Is this model State-of-the-Art?\")\n",
        "#     new_features: Union[List[str],str] = Field(\n",
        "#         description=\"Introduced new model components, layers or other features, as keywords, each seperated by commas\",\n",
        "#     )\n",
        "#     new_strategies: Union[List[str],str]  = Field(\n",
        "#         description=\"New strategies introduced, as keywords\"\n",
        "#     )\n",
        "#     date: str = Field(description=\"Date of the paper\")\n",
        "\n",
        "from templates import ShortInfoSummary\n",
        "\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=ShortInfoSummary)"
      ],
      "metadata": {
        "id": "Fp6xLAZcFWvS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.llm_doc_meta_updater(update_key=\"test\", prompt=\"identify_features\", document_ids=doc_ids[0], output_parser=parser)\n",
        "dataset.get_by_id(doc_ids[0], include=[\"metadatas\"])[\"metadatas\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afVUHXa25bdY",
        "outputId": "5d0e3032-4b1a-439a-b6a5-623f47d4c736"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Updating metadata: 1it [00:01,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf',\n",
              " 'file_path': '/content/Arxiv_GPT_Summarizer/sample_documents/2302.00386.pdf',\n",
              " 'page': 0,\n",
              " 'total_pages': 5,\n",
              " 'format': 'PDF 1.5',\n",
              " 'title': '',\n",
              " 'author': '',\n",
              " 'subject': '',\n",
              " 'keywords': '',\n",
              " 'creator': 'LaTeX with hyperref',\n",
              " 'producer': 'pdfTeX-1.40.21',\n",
              " 'creationDate': 'D:20230202013154Z',\n",
              " 'modDate': 'D:20230202013154Z',\n",
              " 'trapped': '',\n",
              " 'split_part': 0,\n",
              " 'test.model_name': 'EfﬁcientRep',\n",
              " 'test.model_category': 'Object Detection',\n",
              " 'test.sota': 0,\n",
              " 'test.new_features': \"['Repvgg-style architecture', 'Hardware-aware neural network design']\",\n",
              " 'test.new_strategies': \"['Hardware-aware neural network design']\",\n",
              " 'test.date': '2021',\n",
              " 'test.cls._type': 'ShortInfoSummary',\n",
              " 'test': 'metadata keys [test.cls._type, test.model_name, test.model_category, test.sota, test.new_features, test.new_strategies, test.date]'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import get_document_name\n",
        "\n",
        "result, source_docs = dataset.llm_search(\"tell me about yolov7\", chain_type=\"map_reduce\", return_source_documents=True)\n",
        "print('\\033[92m', \"Answer:\", '\\033[0m', result, \"\\n\")\n",
        "\n",
        "print('\\033[92m', \"Source Documents:\", '\\033[0m' )\n",
        "for doc in source_docs:\n",
        "  print(get_document_name(doc))\n",
        "# [doc.metadata for doc in source_docs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDZFbUt58hen",
        "outputId": "b659fe92-344b-45af-f2cf-9b465e2d3643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m Answer: \u001b[0m YOLOv7 is a model developed by Chien-Yao Wang and Alexey Bochkovskiy et al. in 2022. It integrates strategies such as E-ELAN, model scaling, and model re-parameterization to achieve a balance between detection efficiency and precision. The network consists of four main modules: Input, Backbone, Head, and Prediction. The Input module uses mosaic and hybrid data enhancement techniques, while the Backbone network includes components like CBS, E-ELAN, and MP1. The Head network utilizes the Feature Pyramid Network architecture. YOLOv7 has shown superior performance in speed and accuracy compared to other object detectors, achieving the highest accuracy of 56.8% AP on test-dev / 56.8% AP min-val among real-time object detectors with 30 FPS or higher on GPU V100. \n",
            "\n",
            "\u001b[92m Source Documents: \u001b[0m\n",
            "2302.00386.pdf - page: 3, part: 14\n",
            "Underwater target detection based on improved YOLOv7 - page: 0, part: 5\n",
            "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 27\n",
            "2302.00386.pdf - page: 4, part: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.search_by_field(\n",
        "  field_name=\"source\",\n",
        "  search_value=\"*pdf\",\n",
        "  regex_match=True,\n",
        "  include = [\"embeddings\"])\n",
        "# summarize_paper"
      ],
      "metadata": {
        "id": "Fink4j7UzNlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989a07ad-5e62-4278-f7fb-149e461bf549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Searching documents: 100%|██████████| 136/136 [00:00<00:00, 164435.09it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': [], 'embeddings': [], 'documents': None, 'metadatas': None}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[ get_document_name(d) for d in dataset.search_by_name('real-time object detectors', regex_match=True, include=[\"metadatas\"])['metadatas']]"
      ],
      "metadata": {
        "id": "WV1KLcso1PaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2c88a1-d4c2-42a6-d4dd-80f8cf2c880d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Searching documents: 100%|██████████| 136/136 [00:00<00:00, 127497.84it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 37',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 20',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 21',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 22',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 23',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 24',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 25',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 26',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 27',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 28',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 19',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 30',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 31',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 32',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 33',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 34',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 35',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 36',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 10',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 1',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 29',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 2',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 3',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 4',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 5',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 6',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 7',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 8',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 9',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 0',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 11',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 12',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 13',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 14',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 15',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 16',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 17',\n",
              " 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors - page: 0, part: 18']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# search document by part of name\n",
        "found_docs = dataset.search_by_name(\"2302.00386\", include=[\"metadatas\"], regex_match=True)\n",
        "for doc_id, doc_meta in zip(found_docs[\"ids\"], found_docs[\"metadatas\"]):\n",
        "  print(doc_id, \" : \", get_document_name(doc_meta))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1Zd3dVycDqB",
        "outputId": "b5585250-674d-4617-b676-97404095267f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Searching documents: 100%|██████████| 136/136 [00:00<00:00, 106244.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6b4c8e36-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 0, part: 2\n",
            "6b4c8eb8-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 1, part: 3\n",
            "6b4c8f3a-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 1, part: 4\n",
            "6b4c8fb2-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 1, part: 5\n",
            "6b4c9020-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 1, part: 6\n",
            "6b4c9098-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 1, part: 7\n",
            "6b4c9106-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 2, part: 8\n",
            "6b4c9174-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 2, part: 9\n",
            "6b4c91ec-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 2, part: 10\n",
            "6b4c925a-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 2, part: 11\n",
            "6b4c92c8-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 3, part: 12\n",
            "6b4c9336-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 3, part: 13\n",
            "6b4c93a4-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 3, part: 14\n",
            "6b4c9412-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 4, part: 15\n",
            "6b4c8d96-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 0, part: 1\n",
            "6b4c8b8e-d365-11ee-aee1-0242ac1c000c  :  2302.00386.pdf - page: 0, part: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Examples"
      ],
      "metadata": {
        "id": "IbI0Zee9Redg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Save Paper and summary in Google Drive or download"
      ],
      "metadata": {
        "id": "kIoOvM4RpPQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download"
      ],
      "metadata": {
        "id": "Yc14x1cEpeA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "for p in DOWNLOADED_PAPERS:\n",
        "  zipfile = os.path.splitext(os.path.basename(p))[0].split('.')[-1] + \".zip\"\n",
        "\n",
        "  if os.path.isfile(zipfile):\n",
        "    files.download(zipfile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CyVV6iuPpnIB",
        "outputId": "6aa1061f-c3db-4fe4-9bbe-7226d85ad4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2c76e7f9-232e-417f-b85c-c4f3e92e1534\", \"YOLOv6_v3_0_A_Full_Scale_Reloading.zip\", 561420)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Move to Google drive"
      ],
      "metadata": {
        "id": "WIhlY_tRqOcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "GOOGLE_DRIVE_OUTPUT_DIRECTORY = \"/content/drive/MyDrive/\"\n",
        "for p in DOWNLOADED_PAPERS:\n",
        "  zipfile = os.path.splitext(os.path.basename(p))[0].split('.')[-1] + \".zip\"\n",
        "\n",
        "  if os.path.isfile(zipfile):\n",
        "    move(zipfile, os.path.join(GOOGLE_DRIVE_OUTPUT_DIRECTORY, zipfile))\n",
        "    print(f\"Moved {zipfile} to {GOOGLE_DRIVE_OUTPUT_DIRECTORY}\")"
      ],
      "metadata": {
        "id": "-xgjIFSTqRH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Download short summary"
      ],
      "metadata": {
        "id": "Hle6_Mbmrnk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"short_summary.json\")"
      ],
      "metadata": {
        "id": "Iz4wGUQCrrRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}